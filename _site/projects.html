<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf-8">
    <title>Projects - Luke Dahl</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@700&display=swap" rel="stylesheet"> 
    <link rel="stylesheet" href="/lukedahl/assets/css/main.css">
  </head>
  <body>
    <!-- <header>Luke Dahl</header> -->
    <section class="page-wrap">
      <nav>
        <header><h2>Luke Dahl</h2></header>
        <!-- <hr /> -->
        <ul>
          
          
            
              <li><a href="/lukedahl/">About</a></li>
            
          
            
              <li class="active"><a href="/lukedahl/projects.html" class="active">Projects</a></li>
            
          
            
              <li><a href="/lukedahl/publications.html">Publications</a></li>
            
          
            
              <li><a href="/lukedahl/resume.html">Resume</a></li>
            
          
            
              <li><a href="/lukedahl/music.html">Music</a></li>
            
          
        </ul>
      </nav>
      <article>
        <h1 id="projects">Projects</h1>
        <div class="content-wrap">
          <h3 id="musical-interfaces-for-novices-exploring-structured-interactions">Musical Interfaces for Novices: Exploring Structured Interactions</h3>

<p>Making music with others is great fun for those with musical skills. But how can we make it accessible to people without musical training? Start with a simple interface for making musical sound. But once you know <em>how</em> to play, how do you decide <em>what</em> to play?</p>

<p>In this HCI experiment, conducted by myself and Sebastien Robaszkiewicz, we created a simple touch-based instrument where two users play duets together. To this we added a game-like interaction which directs players to different parts of the “musical space” at different times. We compared the users’ experience and the musical results between these “structured interactions”, and unstructured free play.</p>

<p>The results are discussed in the <a href="/assets/pdfs/Dahl&amp;Robaszkiewicz-UIST2012.pdf">abstract</a> for our 2012 UIST poster.</p>

<p>Here’s a demo of the system in action:</p>

<iframe src="//player.vimeo.com/video/46977021" width="580" height="326" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
<p><a href="http://vimeo.com/46977021">Structuring Musical Interactions for Novices - UIST 2012 Demo</a></p>

<hr />

<h3 id="launchcode-integrating-the-launchpad-and-code-in-ableton-live">LaunchCode: Integrating the Launchpad and Code in Ableton Live</h3>

<p>LaunchCode is a set of python scripts which allows the Novation Launchpad and Livid Code controllers to work together as an integrated controller for Ableton Live. With LaunchCode the Launchpad and Code use the same session box (the yellow box in Live’s session view), and both address the same eight tracks encompassed by the session box. The idea is to use the Launchpad to launch clips and scenes and the Code to control mixer and device parameters. Details and code are available on the <a href="http://morethanreal.github.com/LaunchCode/">GitHub page</a>.</p>

<p><img src="/lukedahl/assets/images/LaunchCode.png" alt="LaunchPad+Code=LaunchCode" /></p>

<hr />

<h3 id="tweetdreams-music-for-live-twitter-data">TweetDreams: Music for live Twitter data</h3>

<p><em>TweetDreams</em> is a multimedia music performance piece that uses real-time Twitter data. The software retrieves from Twitter all currently occuring tweets containing certain terms. The tweets are sonified as short melodies and displayed graphically. Similar tweets are grouped into trees of similar melodies, and new tweets cause neighboring tweets to also play their melodies, creating percolating textures of musical tweets.</p>

<p><em>TweetDreams</em> is designed to allow the audience to participate in the music-making process. During a performance the audience is invited to tweet during, and their tweets are given special musical and visual prominence.</p>

<p>Additionally, during a performance anyone in the world who tweets with one of the search terms becomes an unwitting participant in the piece. In this way <em>TweetDreams</em> creates musical interactions which are both local and global.</p>

<iframe src="//player.vimeo.com/video/81179236" width="580" height="326" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>

<p><em>TweetDreams</em> was developed by myself, Jorge Herrera, and Carr Wilkerson. It was premiered at the MiTo Settembre Musica festival in Milan in 2010, and has been performed at a number of events including <a href="http://www.tedxsv.org/">TEDxSV</a>, and <a href="http://nime2011.org/">NIME 2011</a> in Oslo.</p>

<p>For performance videos checkout the <a href="http://ccrma.stanford.edu/groups/tweetdreams/"><em>TweetDreams</em> project page</a>, and for more technical details read our <a href="../pdfs/Dahl,Herrera,Wilkerson-TweetDreams-Nime2011.pdf">NIME paper</a>.</p>

<p>Here’s me discussing the piece at NIME 2011:</p>

<iframe width="580" height="326" src="//www.youtube.com/embed/nBSdi7CWZC4?feature=player_embedded" frameborder="0" allowfullscreen=""></iframe>

<hr />

<h3 id="soundbounce-throwing-sounds-around-with-iphones">SoundBounce: Throwing sounds around with iPhones</h3>

<p><em>SoundBounce</em> is an instrument I developed for the Stanford Mobile Phone Orchestra. It is implemented as an iPhone app in which players must “bounce” sounds with an upward paddling motion to keep them alive. They can also throw sounds to other players by pointing at them and making a throwing gesture.</p>

<p>A performance of <em>SoundBounce</em> involves the performers bouncing sounds and throwing them amongst the ensemble in various patterns. The piece ends with a <em>game</em> in which performers can knock out other performer’s sounds, and the goal is to be the last person standing.</p>

<p><em>SoundBounce</em> was premiered at the <a href="http://mopho.stanford.edu/events/2009/i-mopho/">i, MoPho</a> concert at CCRMA, December 2009, and has been played elswhere, including <a href="http://nime2010.org/">NIME 2010</a> in Sydney.</p>

<p>Here’s a video of the premiere performance (thanks to <a href="http://mashable.com/2009/12/06/stanford-mobile-phone-orchestra/">mashable</a>):</p>

<iframe width="580" height="326" src="//www.youtube.com/embed/qVj5OGDxGZI" frameborder="0" allowfullscreen=""></iframe>

<p>Technical details are available in my <a href="../pdfs/Dahl,Wang-SoundBounce-Nime2010.pdf">NIME paper</a>.</p>

<hr />

<h3 id="movement-sonification">Movement Sonification</h3>

<p>I’ve created a number of sonifications of human movement. Here’s a demonstration of sonifying muscle activation in running and walking:</p>

<iframe src="//player.vimeo.com/video/35406943" width="580" height="326" frameborder="0" allowfullscreen=""></iframe>
<p><a href="http://vimeo.com/35406943">Sonification of Running</a></p>

<hr />

<h3 id="the-wavesaw-a-flexible-instrument-for-direct-timbral-manipulation">The WaveSaw: A flexible instrument for direct timbral manipulation</h3>

<p>The WaveSaw is a new instrument designed as a class project at CCRMA with myself, Nate Whetsell, and John Van Stoecker. Bend sensors on the “blade” are used to recreate the approximate shape of the blade in the computer. This shape is used as one period of an audio waveform (a.k.a. “scanned synthesis”), and it can also be used to shape the spectrum of that sound. Add in multiple “reflections” of the shape, and timbral insanity ensues.</p>

<p><img src="/lukedahl/assets/images/LDwaveSaw.jpg" alt="me with the wavesaw" /></p>

<p>Here’s a demo of the WaveSaw and its features:</p>

<iframe src="//player.vimeo.com/video/46390800" width="580" height="326" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe>
<p><a href="http://vimeo.com/46390800">The WaveSaw: A Flexible Instrument for Direct Timbral Manipulation</a></p>

<p>For more details see our <a href="../pdfs/Dahl,Whetsell,vanStoecker-Wavesaw-Nime2007.pdf">2007 NIME paper</a></p>

        </div>
      </article>
    </section>
  </body>
</html>
